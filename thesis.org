#+TODO: TODO UPNEXT DOING REVIEW DONE

* About this document / setup

* Inbox
** DONE thesis declaration
CLOSED: [2019-01-28 Mon 21:11]
 added on [2018-12-26 Wed 13:42]
** function interaction 
Most challenging part of the process is communicating the result of thousands of different tests to the user. It's better to give results in terms of function behavior, and group functions with common characteristics and give guidelines of writing tests for each group.
*** possible groups
It should be obvious to determine getters and setters of the same member, and then decide how others interact
**** does not affect other functions
mostly getters. There has to be some spectrum of how much the function mutates the structure. But these ones will be affected by others.
**** affects other functions
***** how many?
***** what is the change in behavior
***** maybe the affected ones form the group themselves?
**** behavior on repeated calls
**** input dependent
how different inputs change function behavior and interaction with other functions
*** how to measure 
each test will contain a group of functions. Get a set and update results for each. But each will have a different context in that particular sequence. And maybe it appears several times and each one gives us new information that is not connected to others -> will need another sequence result to find out. 
**** types of information gathered
***** during the execution
***** after the execution (connecting the dots)
*** challenges
Will need to examine existing codebases to detect patterns
** https://mcss.mosra.cz/doxygen/
* meetings
** <2018-11-12 Tue 13:39>
*** summary 
After going through
** <2018-11-26 Mon 13:40>
*** summary
First draft
** <2018-12-13 Thu 13:40>
*** summary
created the declaration of thesis
*** decisions
**** will use clang code coverage instead of libfuzz
My arguments:
***** don't need randomized input
is that for combinations, there is a finite amount of sets we could test, which will be a subset of a much larger input set that libfuzz might provide us with (ie useless filtering of garbage)
***** better coverage info
libfuzz will filter out code coverage information that is irrelevant for APIs but might be crucial in terms of function combinations 
****** TODO examples
***** extracting information
More control over combination hierarchy, no need to go through trouble of extracting it manually from libfuzz corpus (don't even know if it's possible in full form)
** <2019-02-26 Tue 15:08> 
*** report
**** tried llvm-cov and then found sanitizerCoverage
exactly what we needed and also what libfuzzer uses
**** fixed functionPointerMap
was forgetting to pass the class instance, then forgetting to pass by reference
**** created CombinationGenerator
doesn't store all combinations, insted just indexes
would be suitable for multithreading too
*** discussion topics
**** current issues
***** storing function arguments
since functions are cast to void, I can't use decltype anywhere. everything has to be excplicitly specified. Right now I'm just ignoring the return value (technically do we need it?)
should I use boost::any?
std::any in c++17 or std::variant (better) also http://gsd.web.elte.hu/gyak/cpp17util.pdf
***** coverage_pc does not work for template files
pc_guards are inserted when creating the object file, otherwise it inserts the guards aaaall over the place. 
**** design decisions
***** what information to extract during processing
right now it's recorded by function call (simple bool started) maybe I should record guards by each function in the sequence and see how calling other functions affects its guards 
***** [[function interaction][how to analyze coverage]]
**** what's next
***** start implementing [[design decisions]]
***** start learning clang AST
***** try to use threads
for 5 most primitive functions and 8 combinations, it takes 5 seconds already 
**** misc tasks
***** start documentation
***** makefile
***** gdb
***** more tests
* Presentation
** process description
*** store function pointers in a map
*** collect coverage
**** initially thought to use libfuzzer
[[some differences from fuzzing]]
**** explored other paths
***** llvm-cov
summarizes the intire run, so if I call three different combinations of functions they results will be mixed in together. But it has some nice visualization data and maybe that could be used to display the results
looked into libfuzzer source code to find out what they where using to collect the coverage, since they were doing multiple "isolated" runs to analyze
***** sanitizercoverage
****** how it works
guards are inserted at function entrance points, if/else block starts, etc. You can implement the functions that get called when the guard is initialized and then when it's called
****** how I use it
there is one main function that has the map with all the functions
*** analyze coverage
**** base assumptions
***** order of calls does not matter
***** 
** next steps
*** 
** ...
*** having an additional argument
make some measurements
**** take std::vector
and ignore some of the functions, for example insert, erase etc, and just use push_back, pop_back;
**** maybe deck too
compare what was the behavior of the stuff
**** user manual, developer manual, test cases
** ...
*** grammar induction
exhaustive search on relatively small input to infer the grammar. 
*** generational algorithm
the way it overcomes the gaps 
mutations like appending to random sequences together 
*** contact libfuzzer developers
** some differences from fuzzing
*** coverage info
libfuzzer is created with different intent in mind and extracting the coverage information which is already filtered according to the library's priority might not be what we want
*** possible input space
much larger in fuzzing, more limited (and deterministic) here
*** path exploration
since it's more feasible to keep exploring different paths (for example calling the same function 17th time will unlock a new pc block) 
*** handling exceptions
fuzzer will hault on first exception it finds, because the design philosophy is that the consumer is an API. In case of libraries, exceptions might be expected so here each function (or combination?) call will be wrapped in a try catch so all the different call sequences that result in various exceptions
* Project outline
** Motivation
start with linear number of test cases but at some point it explodes. Hard to determine which test cases are meaningful. 
** About fuzzing
** What can't be covered with fuzzing
*** Does not consider the interaction of different functions
If you write a TDD application, there is a well defined interface of functions.
*** APIs should be tolerable
(reference cppcon 17 video)
Any kind of crash/abort/assert failure/timeout is considered a bug in an API, whereas for libraries it could be expected behavior that should be covered in tests
** ..
*** why we're considering every possible input
there might be new code coverage anywhere, and since it is a finite set...
* Research / Learning
** DOING c++ tutorials
general knowledge of language since I don't have a lot of experience currently
** clang
*** DOING understand code coverage library
*** DOING get familiar with libfuzz source code
understand how libfuzz works since a lot of mechanisms are similar

*** TODO AST
will be needed for extracting type information
** c++ reference
*** typeinfo
**** typeid
Used where the dynamic type of a polymorphic object must be known and for static type identification. The typeid expression is an lvalue expression which refers to an object with static storage duration, of the polymorphic type const std::type_info or of some type derived from it.
result refers to [[type_info]]
**** type_info
The class type_info holds implementation-specific information about a type, including the name of the type and means to compare two types for equality or collating order. This is the class returned by the [[typeid]] operator.
**** type_index 
The type_index class is a wrapper class around a std::type_info object, that can be used as index in associative and unordered associative containers. The relationship with type_info object is maintained through a pointer
* Process description
** Analyze the library
*** TODO what information can be inferred automatically?
What will be the manual tasks that the programmer will need to do and specify for the library to work
**** Type information
should be able to do with clang
** Generate 

** Run coverage tests
*** TODO how will the function inputs be handled?
- my idea is to test each of the functions using libfuzz (would need to somehow get the output still)
- it might be best for the programmer to provide 
** Analyze and communicate the results
*** TODO give the smallest possible subset of function combinations
*** TODO extra information
- What else does the interaction of functions tell us?
- Can we predict possible problems with the function based on code coverage
* commands and stuff
** llvm-cov
https://clang.llvm.org/docs/SourceBasedCodeCoverage.html

#+BEGIN-EXAMPLE
clang++ -fprofile-instr-generate -fcoverage-mapping stack.cpp -o stack
LLVM_PROFILE_FILE="stack.profraw" ./stack
llvm-profdata merge -sparse stack.profraw -o stack.profdata
llvm-cov show ./stack -instr-profile=stack.profdata
llvm-cov report ./stack -instr-profile=stack.profdata
llvm-cov export ./stack -instr-profile=stack.profdata > export.json
#+END-EXAMPLE
*** flags
**** sparse
The -sparse flag is optional but can result in dramatically smaller indexed profiles. This option should not be used if the indexed profile will be reused for PGO.
* related work
** klee 
http://klee.github.io
papers
*** KLEE: Unassisted and Automatic Generation of High-CoverageTests for Complex Systems Programs
http://www.doc.ic.ac.uk/~cristic/papers/klee-osdi-08.pdf
*** Abstract
We  present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage — on average over 90 %per tool (median: over 94%) — and significantly beat the coverage of the developers’ own hand-written testsuite. When we did the same for 75 equivalent tools inthe BUSYBOX embedded system suite, results were evenbetter, including 100% coverage on 31 of them.
We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.
*** difference
operates on bytecode instead of c++, which means the results cannot be easily implemented in ci or sth
** KLOVER: A Symbolic Execution and AutomaticTest Generation Tool for C++ Programs
http://www.cs.utah.edu/~ligd/publications/KLOVER-CAV11.pdf
*** abstract
We present the first symbolic execution and automatic testgeneration tool for C++ programs. First we describe our effortin extend-ing an existing symbolic execution tool for C programs to handleC++programs. We then show how we made this tool generic, efficientandusable to handle real-life industrial applications. Novelfeatures includeextended symbolic virtual machine, library optimization for Cand C++,object-level execution and reasoning, interfacing with specific type of ef-ficient solvers, and semi-automatic unit and component testing. This toolis being used to assist the validation and testing of industrial softwareas well as publicly available programs written using the C++ language
*** notes
As shown in Fig. 1, the tool’s flow is similar to KLEE’s. A C++ program is compiled into LLVM bytecode, which is interpreted by KLOVER for symbolic execution
